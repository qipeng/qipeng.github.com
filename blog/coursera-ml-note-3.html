<html lang="en">
<head>
    <meta charset="utf-8">

  <!-- Always force latest IE rendering engine (even in intranet) & Chrome Frame
       Remove this if you use the .htaccess -->
  <meta http-equiv="X-UA-Compatible" content="">
  <meta http-equiv="Content-Type" content="text/html; charset=utf8" />
  <meta name="robots" content="all" />
  <meta name="googlebot" content="all" />

  <title>[Coursera] Machine Learning Notes - Week 7-10</title>
  <meta name="description" content="Course Notes for machine learning course offered by Andrew Ng at coursera.org
">
  <meta name="keywords" content="">
  <meta name="author" content="Peng Qi">

  <!--  Mobile viewport optimized: j.mp/bplateviewport -->
  <meta name="viewport" content="">

    <link rel="shortcut icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/media/css/site.css" type="text/css">
  <link rel="stylesheet" href="/media/css/syntax.css" type="text/css">
  <link rel="stylesheet" href="/media/js/fancybox/jquery.fancybox-1.3.4.css" type="text/css">
  <link rel="stylesheet" href="/media/js/nivo-slider/themes/light/light.css" type="text/css" media="screen" />
  <link rel="stylesheet" href="/media/js/nivo-slider/nivo-slider.css" type="text/css" media="screen" />
      <!-- All JavaScript at the bottom, except for Modernizr which
        enables HTML5 elements & feature detects -->
    <script type="text/javascript" src="/media/js/libs/modernizr-1.7.min.js"></script>
    </head>
<body id="coursera-ml-note-3">
    <div id="container">
            <div id="main" role="main">
          <header class="banner clearfix">
          <h1>Peng Qi</h1>                              <nav class="main_nav">
    <ul>
                <li>
            <a title="Home Page"
                class="button home"
                href="/index.html">
                Home
            </a>
        </li>        <li>
            <a title="Research"
                class="button research"
                href="/research">
                Research
            </a>
        </li>        <li>
            <a title="Blog"
                class="button active blog"
                href="/blog">
                Blog
            </a>
        </li>        <li>
            <a title="Software"
                class="button software"
                href="/software">
                Software
            </a>
        </li>        <li>
            <a title="Misc"
                class="button misc"
                href="/misc">
                Me &amp; My ...
            </a>
        </li>    </ul>
</nav>
                              </header><br clear="both">
          <div class="content-back">
          <div class="content-right">
          <div class="right-float">
<span class="lighter" >Tags:</span><br/>
<ul class="tags clear">
<li>
    <a class="small" href="/blog/tags/coursera.html">
        coursera
    </a>
</li>
<li>
    <a class="small" href="/blog/tags/machine_learning.html">
        machine_learning
    </a>
</li>
<li>
    <a class="small" href="/blog/tags/note.html">
        note
    </a>
</li>
</ul>
</div>
          <div class="right-float"><div id="sociallinks" style="font-size:10px;">
          <div id="googletranslate" style="position:absolute;right:20px;bottom:5px"></div>
          </div></div>
          </div>
          <div class="contentwrapper">
          <section class="content">
          <article class="post">

<a href="/blog">Blog</a> &nbsp;&gt;&nbsp; [Coursera] Machine Learning Notes - Week 7-10<br/>
<nav class="postnav before">
Previous:
<a href="/blog/lecture-notes-201212.html">Lecture Notes from Three Recent Lectures</a>
<br/>

Next:
<a href="/blog/coursera-ml-note-2.html">[Coursera] Machine Learning Notes - Week 4-6</a>
</nav>

<div class="title"><h1 >
    [Coursera] Machine Learning Notes - Week 7-10
</h1></div>
<time datetime="2012-10-21">
    Posted: Sun, 21 Oct 2012
</time>

<div class="postcontent">
<p>
This post are the fresh notes of the current offering of Machine Learning course
on <a href="https://coursera.org" target="_blank">coursera.org</a>, which covers the
courses offered in Week 7 (Support Vector Machines) 
through Week 10 (Large-scale Machine&nbsp;Learning).</p>

<p>
So here are the notes from Week&nbsp;7-10.
</p>

<p>
<div class='title'><h2>Week 7: Support Vector&nbsp;Machines</h2></div>
<ul class='news'>
<li>The basic <span class="lighter">linear <span class="caps">SVM</span></span> is obtained by replacing the log difference in the cost function with a rectified linear function (but with an offset of 1 on both&nbsp;sides).</li>
<li>The offset in the cost function can count for the property of <span class="lighter">large margine</span>, which helps the classifier to generalize better by choosing a decision boundary of large margin from positive and negative examples. Such behavior can be tuned by the C factor in the cost function, leveraging between high bias and high&nbsp;variance.</li>
<li><span class="lighter">Kernels</span> can extend the application of support vector machines. Kernels are usually similarity measures between the input data vectors and chosen <span class="lighter">landmark vectors</span>. If the similarity function is, for example, Gaussian centered at the landmarks, the resulting <span class="caps">SVM</span> is called a Gaussian&nbsp;<span class="caps">SVM</span>.</li>
</ul>
</p>

<p>
<div class='title'><h2>Week 8: Clustering and Dimensionality&nbsp;Reduction</h2></div>
<ul class='news'>
<li><span class="lighter">Clustering</span> and obtaining a good inner representation are two important forms of unsupervised&nbsp;learning.</li>
<li><span class="lighter">K-means</span> is a clustering algorithm that takes an iterative approach between &#8220;center assignment&#8221; steps and &#8220;center re-location&#8221; steps. During a center assignmnet step, each data point is assigned to the nearest of the K centroids; and during a center re-location step, each of the K centroids is re-located at the arithmetic mean of the data points that&#8217;s assigned to the&nbsp;centroid.</li>
<li>K-means centroids should usually be <span class="lighter">randomly initialized</span> as random chosen data points. And one should expect to run K-means for several times to obtain stable results, with independent (different) random&nbsp;initializations.</li>
<li><span class="lighter">The choice of the number of clusters</span> should preferrably be determined by downstream purpose of the clustering, e.g. the profit of T-shirts given different number of sizes. Otherwise, one could plot the total clustering cost (sum of squared distances from data points to clustering centroids), and choose the value where the derivative of the error diminishes significantly (this is called the <span class="lighter">&#8220;Elbow method&#8221;</span>).</li>
<li>Two apparent motivations for <span class="lighter">dimensionality reduction</span> are data compression (for storage or computation speed-up purposes) and data visualization (for higher-dimensional&nbsp;data). </li>
<li><span class="lighter">Principle Component Analysis (<span class="caps">PCA</span>)</span> is one of the most commonly used algorithm for dimensionality reduction. It computes the eigen-decomposition of the covariance matrix of the data, and then takes only the eigenvectors corresponding to the largest K eigenvalues to represent the data (by projecting the data points on to the subspace spanned by such&nbsp;vectors).</li>
<li>The number of principle components to keep should usually retain 90 or higher percent (preferably 99%) of the variance of the data, meaning the sum of squared difference between the reconstructed data divided by the sum of squared data&nbsp;(dimension-wise).</li>
</ul>
</p>

<p>
<div class='title'><h2>Week 9: Anomaly Detection and Recommender&nbsp;Systems</h2></div>
<ul class='news'>
<li><span class="lighter">Anomaly detection</span> is a kind of unsupervised learning, in which normal data is usually modeled by a Gaussian distribution, and abnormal data points are distinguished by a lower probability under this distribution below some threshold epsilon. This algorithms is especially suitable for problems where the classes are very skewed, i.e. only a few positive examples are available, while a lot of negative ones are present. This algorithm is also useful where the properties of anomalies are not fully captured by training&nbsp;data.</li>
<li><span class="lighter">Multivariate Gaussian</span> is very useful in anomaly dectection when there&#8217;s correlation between input dimensions. However it&#8217;s more computationally costly, and takes more memory space&nbsp;too.</li>
<li><span class="lighter">Collaborative filtering</span> is an important algorithm for <span class="lighter">recommender systems</span>. Collaborative filtering can be implemented via low rank matrix factorization, where the rankings matrix Y is decomposed into a parameter matrix theta (where each row denotes the ranking of a specific feature by a user), and a feature matrix X (where each column denotes the portion of features of some given content). Gradient descent on both matrices is the recommended algorithm to learn such&nbsp;factorization</li>
<li>It is worth notice that, when implementing collaborative filtering, it is necessary to do mean normalization&nbsp;first.</li>
</ul>
</p>

<p>
<div class='title'><h2>Week 10: Large Scale Machine&nbsp;Learning</h2></div>
<ul class='news'>
<li><span class="lighter">Larger dataset</span> is often the key to success for many modern learning&nbsp;systems.</li>
<li><span class="lighter">Stochastic gradient descent</span> runs through the dataset one example by one example, and computes the gradient for each data point to run descent. Random shuffling the dataset is&nbsp;necessary.</li>
<li><span class="lighter">Mini-batch gradient descent</span> lies somewhere between stochastic gradient descent and batch gradient descent (where all the data are taken into consideration for each gradient descent step). It takes a small portion of data (but more than 1), and compute the average gradient for descent. Random shuffling is also necessary, and mini-batch gradient descent is usually much more stable than stochastic gradient descent, thus the recommended algorithm to&nbsp;use.</li>
<li><span class="lighter">Online learning</span> is quite similar to stochastic gradient descent, in that it takes one sample at a time and computes the gradient. The major difference is that online learning does not keep the dataset; on the opposite, online learning discards every datum that it&#8217;s seen and moves on to new data. This allows online learning to enjoy the advantage of self-adaptation to time-changing&nbsp;data.</li>
<li><span class="lighter">Map-reduce</span> is an important method for making use of both computational and storage resources, by first splitting up task to different processing units or even different computers (map), then collect together the results from each processing unit / computer to calculate the final result (reduce).
</ul>
</p>

<p>
<div class='title'><h2>See&nbsp;Also&#8230;</h2></div>
<a href="coursera-ml-note-1.html">[Coursera] Machine Learning Notes - Week 1-3</a><br>
<a href="coursera-ml-note-2.html">[Coursera] Machine Learning Notes - Week&nbsp;4-6</a>
</p></div>

<nav class="postnav after">
Previous:
<a href="/blog/lecture-notes-201212.html">Lecture Notes from Three Recent Lectures</a>
<br/>

Next:
<a href="/blog/coursera-ml-note-2.html">[Coursera] Machine Learning Notes - Week 4-6</a>
</nav>

<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'pengqishomepage'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>
</article>          </section>
          </div>
          </div>
      </div>
      </div> <!--! end of #container -->
  <footer>
      <div id="designlogo"><a href="http://qipeng.github.com/misc/homepage.html" target="_blank"><span style="color:#11bb22;">Designed</span>by<br>
    <span style="padding-left:40px">Qi<span style="color:#11bb22;">Peng</span></span></a>
    <br><span style="font-size:10px;text-align:center;">Powered by <a href="http://hyde.github.com/" target="_blank" class="hyde">hyde</a>.</span></div>
      <p style="text-align:center">&copy; <a href="http://qipeng.github.com/">Peng Qi</a>, 2012. Hosted by <a href="http://www.github.com">GitHub</a>.</p>
  </footer>
    
    <!-- Javascript at the bottom for fast page loading -->
    <!-- Grab Google CDN's jQuery, with a protocol relative URL; fall back to local if necessary -->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.js"></script>
  <script>window.jQuery || document.write('<script src="/media/js/libs/jquery-1.7.2.min.js">\x3C/script>')</script>
  
  <script type="text/javascript" src="/media/js/main.js"></script>
  <script src="/media/js/nivo-slider/jquery.nivo.slider.pack.js" type="text/javascript"></script>
  <script type="text/javascript" src="/media/js/fancybox/jquery.fancybox-1.3.4.js"></script>
  <script type="text/javascript">$(function(){loadimg();loadgoogletranslate();loadsociallinks();});</script>
  <script> $(window).load(function() {$('#slider').nivoSlider({effect: 'sliceDownRight', slices:15, animSpeed: 300, pauseTime: 5000});});</script>
  
  
  <!--[if lt IE 7 ]>
    <script src="/media/js/libs/dd_belatedpng.js"></script>
    <script>DD_belatedPNG.fix('img, .png_bg'); // Fix any <img> or .png_bg bg-images. Also, please read goo.gl/mZiyb </script>
  <![endif]--><script>
    var _gaq = [['_setAccount', 'UA-26770326-1'], ['_trackPageview']];
    (function(d, t) {
    var g = d.createElement(t),
        s = d.getElementsByTagName(t)[0];
    g.async = true;
    g.src = ('https:' == location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g, s);
    })(document, 'script');
</script>
  </body>
</html>